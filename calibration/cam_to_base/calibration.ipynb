{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import open3d as o3d\n",
    "import cv2\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(base_dir):\n",
    "    rgb_folder = os.path.join(base_dir, 'images')\n",
    "    depth_folder = os.path.join(base_dir, 'depth')\n",
    "    pose_folder = os.path.join(base_dir, 'poses')\n",
    "\n",
    "    print(rgb_folder)\n",
    "\n",
    "    rgb_list, depth_list, pose_list = None, None, None\n",
    "\n",
    "    # Check if RGB folder exists\n",
    "    if os.path.exists(rgb_folder):\n",
    "        # Read RGB images\n",
    "        rgb_files = [f for f in os.listdir(rgb_folder) if f.endswith('.png')]\n",
    "        rgb_files.sort()\n",
    "        print(rgb_files)\n",
    "        rgb_list = []\n",
    "        for f in rgb_files:\n",
    "            img = cv2.imread(os.path.join(rgb_folder, f))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            rgb_list.append(img)\n",
    "\n",
    "    # Check if depth folder exists\n",
    "    if os.path.exists(depth_folder):\n",
    "        # Read depth images\n",
    "        depth_files = [f for f in os.listdir(depth_folder) if f.endswith('.npy')]\n",
    "        depth_files.sort()\n",
    "        print(depth_files)\n",
    "        depth_list = [np.load(os.path.join(depth_folder, f)) for f in depth_files]\n",
    "\n",
    "    # Check if pose folder exists\n",
    "    if os.path.exists(pose_folder):\n",
    "        # Read poses\n",
    "        pose_files = [f for f in os.listdir(pose_folder) if f.endswith('.npy')]\n",
    "        pose_files.sort()\n",
    "        print(pose_files)\n",
    "        pose_list = [np.load(os.path.join(pose_folder, f)) for f in pose_files]\n",
    "\n",
    "    # Check if camera parameters exist\n",
    "    rgb_params_file = os.path.join(base_dir, 'rgb_intrinsics.npz')\n",
    "    if os.path.exists(rgb_params_file):\n",
    "        # Load the intrinsic parameters\n",
    "        camera_params = np.load(rgb_params_file)\n",
    "        fx = camera_params['fx']\n",
    "        fy = camera_params['fy']\n",
    "        ppx = camera_params['ppx']\n",
    "        ppy = camera_params['ppy']\n",
    "        rgb_coeffs = camera_params['coeffs']\n",
    "        rgb_intrinsics = np.array([[fx, 0, ppx], [0, fy, ppy], [0, 0, 1]])\n",
    "    else:\n",
    "        rgb_intrinsics, rgb_coeffs = None, None\n",
    "\n",
    "    depth_params_file = os.path.join(base_dir, 'depth_intrinsics.npz')\n",
    "    if os.path.exists(depth_params_file):\n",
    "        # Load the intrinsic parameters\n",
    "        camera_params = np.load(depth_params_file)\n",
    "        fx = camera_params['fx']\n",
    "        fy = camera_params['fy']\n",
    "        ppx = camera_params['ppx']\n",
    "        ppy = camera_params['ppy']\n",
    "        depth_coeffs = camera_params['coeffs']\n",
    "        depth_intrinsics = np.array([[fx, 0, ppx], [0, fy, ppy], [0, 0, 1]])\n",
    "        depth_scale = camera_params['depth_scale']\n",
    "    else:\n",
    "        depth_intrinsics, depth_coeffs, depth_scale = None, None, None\n",
    "\n",
    "    return rgb_list, depth_list, pose_list, rgb_intrinsics, rgb_coeffs, depth_intrinsics, depth_coeffs, depth_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../example_data/images\n",
      "['0.png', '1.png', '2.png', '3.png', '4.png', '5.png', '6.png', '7.png', 'left.png', 'right.png']\n",
      "['0.npy', '1.npy', '2.npy', '3.npy', '4.npy', '5.npy', '6.npy', '7.npy', 'left.npy', 'right.npy']\n",
      "['0.npy', '1.npy', '2.npy', '3.npy', '4.npy', '5.npy', '6.npy', '7.npy']\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"../../example_data\"\n",
    "rgb_list, depth_list, arm_pose_list, rgb_intrinsics, rgb_coeffs, depth_intrinsics, depth_coeffs, depth_scale = read_data(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "BaseImage = collections.namedtuple(\n",
    "    \"Image\", [\"id\", \"qvec\", \"tvec\", \"camera_id\", \"name\", \"xys\", \"point3D_ids\"])\n",
    "\n",
    "def qvec2rotmat(qvec):\n",
    "    return np.array([\n",
    "        [1 - 2 * qvec[2]**2 - 2 * qvec[3]**2,\n",
    "         2 * qvec[1] * qvec[2] - 2 * qvec[0] * qvec[3],\n",
    "         2 * qvec[3] * qvec[1] + 2 * qvec[0] * qvec[2]],\n",
    "        [2 * qvec[1] * qvec[2] + 2 * qvec[0] * qvec[3],\n",
    "         1 - 2 * qvec[1]**2 - 2 * qvec[3]**2,\n",
    "         2 * qvec[2] * qvec[3] - 2 * qvec[0] * qvec[1]],\n",
    "        [2 * qvec[3] * qvec[1] - 2 * qvec[0] * qvec[2],\n",
    "         2 * qvec[2] * qvec[3] + 2 * qvec[0] * qvec[1],\n",
    "         1 - 2 * qvec[1]**2 - 2 * qvec[2]**2]])\n",
    "\n",
    "class Image(BaseImage):\n",
    "    def qvec2rotmat(self):\n",
    "        return qvec2rotmat(self.qvec)\n",
    "\n",
    "\n",
    "def read_extrinsics_text(path):\n",
    "    \"\"\"\n",
    "    Taken from https://github.com/colmap/colmap/blob/dev/scripts/python/read_write_model.py\n",
    "    \"\"\"\n",
    "    images = {}\n",
    "    with open(path, \"r\") as fid:\n",
    "        while True:\n",
    "            line = fid.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            line = line.strip()\n",
    "            if len(line) > 0 and line[0] != \"#\":\n",
    "                elems = line.split()\n",
    "                image_id = int(elems[0])\n",
    "                qvec = np.array(tuple(map(float, elems[1:5])))\n",
    "                tvec = np.array(tuple(map(float, elems[5:8])))\n",
    "                camera_id = int(elems[8])\n",
    "                image_name = elems[9]\n",
    "                elems = fid.readline().split()\n",
    "                xys = np.column_stack([tuple(map(float, elems[0::3])),\n",
    "                                       tuple(map(float, elems[1::3]))])\n",
    "                point3D_ids = np.array(tuple(map(int, elems[2::3])))\n",
    "                images[image_id] = Image(\n",
    "                    id=image_id, qvec=qvec, tvec=tvec,\n",
    "                    camera_id=camera_id, name=image_name,\n",
    "                    xys=xys, point3D_ids=point3D_ids)\n",
    "    return images\n",
    "\n",
    "cameras_extrinsic_file = os.path.join(base_dir, \"sparse/0\", \"images.txt\")\n",
    "cam_extrinsics = read_extrinsics_text(cameras_extrinsic_file)\n",
    "pose_list = [None] * len(cam_extrinsics)\n",
    "# print(cam_extrinsics)\n",
    "for key in cam_extrinsics:\n",
    "    extr = cam_extrinsics[key]\n",
    "    # print(extr.id)\n",
    "    R = np.transpose(qvec2rotmat(extr.qvec))\n",
    "    T = np.array(extr.tvec)\n",
    "    pose = np.eye(4)\n",
    "    pose[:3, :3] = R\n",
    "    pose[:3, 3] = -R @ T\n",
    "    # print(T)\n",
    "    pose_list[extr.id-1] = pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand-in-eye Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rotation matrix: \n",
      "[[-0.01122838 -0.99963805  0.02444781]\n",
      " [ 0.99967286 -0.01178395 -0.02270052]\n",
      " [ 0.0229804   0.02418493  0.99944334]]\n",
      "Translation vector: \n",
      "[[ 0.0762766 ]\n",
      " [-0.03742767]\n",
      " [-0.09128776]]\n"
     ]
    }
   ],
   "source": [
    "R_gripper2base_list = []\n",
    "t_gripper2base_list = []\n",
    "R_target2cam_list = []\n",
    "t_target2cam_list = []\n",
    "\n",
    "for gripper2base, cam2target in zip(arm_pose_list, pose_list):\n",
    "    target2cam = np.linalg.inv(cam2target)\n",
    "    R_gripper2base_list.append(gripper2base[:3, :3])\n",
    "    t_gripper2base_list.append(gripper2base[:3, 3])\n",
    "    R_target2cam_list.append(target2cam[:3, :3])\n",
    "    t_target2cam_list.append(target2cam[:3, 3])\n",
    "\n",
    "R_gripper2base_array = np.array(R_gripper2base_list)\n",
    "t_gripper2base_array = np.array(t_gripper2base_list)\n",
    "R_target2cam_array = np.array(R_target2cam_list)\n",
    "t_target2cam_array = np.array(t_target2cam_list)\n",
    "\n",
    "R_cam2gripper_guess = np.eye(3)\n",
    "t_cam2gripper_guess = np.zeros((3, 1))\n",
    "\n",
    "R_cam2gripper, t_cam2gripper = cv2.calibrateHandEye(\n",
    "    R_gripper2base_array, t_gripper2base_array,\n",
    "    R_target2cam_array, t_target2cam_array,\n",
    "    R_cam2gripper_guess, t_cam2gripper_guess,\n",
    "    method=cv2.CALIB_HAND_EYE_TSAI\n",
    ")\n",
    "\n",
    "print(\"Rotation matrix: \")\n",
    "print(R_cam2gripper)\n",
    "print(\"Translation vector: \")\n",
    "print(t_cam2gripper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hand to eye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world2base = np.array([\n",
      "[-0.40905141069568013, -0.836520843489356, 0.3645679934055257, 0.4208515104751188],\n",
      "[-0.9105637149394369, 0.34809783645984305, -0.2229385953298997, 0.20878625894630023],\n",
      "[0.059587452064729984, -0.42315573334156026, -0.9040954379359518, 0.5595708334282338],\n",
      "[0.0, 0.0, 0.0, 1.0],\n",
      "])\n"
     ]
    }
   ],
   "source": [
    "# calculate world2base\n",
    "world2base_list = []\n",
    "for i in range(0,len(rgb_list)-2, 1):\n",
    "    # transform to world frame\n",
    "    extr = cam_extrinsics[i+1]\n",
    "    world2cam = np.eye(4)\n",
    "    world2cam[:3, :3] = qvec2rotmat(extr.qvec)\n",
    "    world2cam[:3, 3] = np.array(extr.tvec)\n",
    "\n",
    "    cam2gripper = np.eye(4)\n",
    "    cam2gripper[:3, :3] = R_cam2gripper\n",
    "    cam2gripper[:3, 3] = t_cam2gripper.flatten()\n",
    "\n",
    "    gripper2base = arm_pose_list[i]\n",
    "    world2base = gripper2base @ cam2gripper @ world2cam\n",
    "\n",
    "    world2base_list.append(world2base)\n",
    "# average the world2base\n",
    "rot_avg = np.zeros((3,3))\n",
    "t_avg = np.zeros((3,1))\n",
    "for i in range(len(world2base_list)):\n",
    "    rot_avg += world2base_list[i][:3,:3]\n",
    "    t_avg += world2base_list[i][:3,3].reshape(3,1)\n",
    "U, S, Vt = np.linalg.svd(rot_avg)\n",
    "rot_avg = U @ Vt\n",
    "t_avg /= len(world2base_list)\n",
    "world2base_avg = np.eye(4)\n",
    "world2base_avg[:3,:3] = rot_avg\n",
    "world2base_avg[:3,3] = t_avg.flatten()\n",
    "# print(world2base_avg)\n",
    "# prinw with ,\n",
    "print(\"world2base = np.array([\")\n",
    "for i in range(4):\n",
    "    print(f\"[{world2base_avg[i,0]}, {world2base_avg[i,1]}, {world2base_avg[i,2]}, {world2base_avg[i,3]}],\")\n",
    "print(\"])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left camera to base\n",
      "[[-0.99770005  0.01623675 -0.06581019  0.48013568]\n",
      " [ 0.05871091  0.692229   -0.71928578  0.64670775]\n",
      " [ 0.03387686 -0.72149524 -0.69159018  0.56304982]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "Right camera to base\n",
      "[[ 0.99987616  0.00383817  0.01526218  0.49218238]\n",
      " [-0.01073356 -0.54290837  0.83972335 -0.66244913]\n",
      " [ 0.01150897 -0.83978317 -0.54279993  0.46647677]\n",
      " [ 0.          0.          0.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Iterate over all images and add them to the point cloud\n",
    "for i in range(0, len(rgb_list),1):\n",
    "    rgb_img = rgb_list[i]\n",
    "    depth_img = depth_list[i]\n",
    "    pose2world = pose_list[i]\n",
    "\n",
    "    cam2base = world2base_avg @ pose2world\n",
    "    # only the last two images(left and right) are side cameras\n",
    "    if i == len(rgb_list)-2:\n",
    "        print(\"Left camera to base\")\n",
    "        print(cam2base)\n",
    "    if i == len(rgb_list)-1:\n",
    "        print(\"Right camera to base\")\n",
    "        print(cam2base)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
